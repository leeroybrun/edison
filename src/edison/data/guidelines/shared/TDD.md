# TDD (Test-Driven Development)


## TDD Checklist
- [ ] RED: I wrote a failing test first (and it failed for the right reason)
- [ ] GREEN: I wrote the minimum code to pass the test (no extras)
- [ ] REFACTOR: I improved code while keeping tests green
- [ ] Evidence: test failure and success outputs documented

## RED-GREEN-REFACTOR Cycle
- RED: write a failing test first and confirm it fails for the right reason.
- GREEN: add the minimum code required to make the test passâ€”no extras.
- REFACTOR: improve the code with all tests green, then rerun the full suite.
- Repeat the cycle for every feature/change.

## Core Rules
- Fail first; do not skip the RED step.
- Minimal green code; avoid speculative features.
- Refactor with a full test run before proceeding.
- Coverage â‰¥90% overall; 100% on new or changed files.
- Update tests only to reflect agreed spec/format changes, never just to "make green." 
- Keep output cleanâ€”no console noise.

## Guardrails
- No `.skip` / `.todo` (or equivalents) committed.
- Do not leave debugging logs in tests.

## TDD Troubleshooting

### Test Won't Fail (RED phase)
- **Symptom**: New test passes immediately
- **Cause**: Test may be testing existing functionality or has a bug
- **Fix**: Verify test actually exercises new code; add assertion that must fail

### Test Won't Pass (GREEN phase)
- **Symptom**: Implementation complete but test still fails
- **Causes**: 
  - Assertion mismatch
  - Missing dependency
  - Wrong import path
- **Fix**: Check exact assertion values; verify all imports; run test in isolation

### Refactor Breaks Tests
- **Symptom**: Tests fail after refactoring
- **Cause**: Refactor changed behavior, not just structure
- **Fix**: Refactoring should ONLY change structure, never behavior; revert and try smaller steps

### Flaky Tests
- **Symptom**: Test passes sometimes, fails sometimes
- **Causes**: Timing issues, shared state, external dependencies
- **Fix**: Add explicit waits; isolate test state; mock external services (only at boundaries)

## Patterns

### Pattern 1: Committed Data + Unique IDs

**Problem:** Tests fail randomly due to data collisions

**Solution:** Use unique identifiers and commit test data

1. Generate unique IDs for test entities
2. Commit data to database before assertions
3. Clean up in afterEach/afterAll

### Pattern 2: PostgreSQL Template Pool

**Problem:** Slow test setup due to migrations

**Solution:** Use template databases

1. Create template database once
2. Clone for each test
3. Drop clones after tests

## Commit Tag Requirements
Commits MUST include explicit markers to document TDD compliance:
- `[RED]` tag for commits with failing test (test written before implementation)
- `[GREEN]` tag for commits where tests pass (minimal implementation added)
- `[REFACTOR]` tag for commits with code cleanup (tests still green)

Example commit messages:
```
[RED] Add failing test for user authentication flow
[GREEN] Implement JWT validation to pass auth tests
[REFACTOR] Extract token validation into reusable helper
```

## Evidence Requirements
- Test file created/committed BEFORE implementation file (verify via git history).
- Commits MUST include explicit markers: `[RED]` then `[GREEN]` (in order).
- RED failure documented â†’ GREEN pass documented â†’ REFACTOR documented.
- Provide RED then GREEN markers or commits in order.
- Attach test output showing the failing run and the passing run.
- Include a coverage report for the round.
- Use `edison tasks ready --run` to collect trusted evidence files; if TDD must be skipped, call `edison tasks ready --disable-tdd --reason "justification"` and record the rationale in the QA brief.
- Evidence files must be generated by `edison tasks ready --run` (trusted runner markers + timestamps). Manual fabrication is rejected.
- Readiness guard auto-detects post-training packages from the git diff and blocks if matching Context7 markers (`context7-<pkg>.txt`, HMAC-stamped when enabled) are missing.
- Evidence filenames and required commands come from ConfigManager (see `.edison/_generated/constitutions/ORCHESTRATORS.md`); do not invent your own list.

## TDD Verification Checklist

### For Orchestrator: Verifying TDD Compliance

Before accepting work from a sub-agent, verify:

#### 1. Test-First Evidence
- [ ] Tests exist in appropriate __tests__/ directory
- [ ] Test file created BEFORE implementation (check git history)
- [ ] Tests cover the requirements specified in task

#### 2. Red Phase Evidence
- [ ] Sub-agent showed tests failing initially
- [ ] Failure messages indicate tests were actually testing something
- [ ] No "test.skip" or commented-out tests

#### 3. Green Phase Evidence
- [ ] All tests now passing
- [ ] No tests were removed or weakened to pass
- [ ] Coverage meets minimum threshold (see quality.coverageTarget)

#### 4. Refactor Phase (if applicable)
- [ ] Tests still pass after refactoring
- [ ] Code is cleaner without changing behavior
- [ ] No new functionality added during refactor

### TDD Verification Report Template

```json
{
  "taskId": "<task-id>",
  "tddCompliance": {
    "testFirst": true|false,
    "redPhaseEvidence": "path/to/screenshot or git commit",
    "greenPhaseEvidence": "path/to/screenshot or git commit",
    "refactorPhase": true|false|"not-applicable",
    "coveragePercent": 85,
    "coverageThreshold": 80,
    "coverageMet": true
  },
  "testSummary": {
    "total": 12,
    "passed": 12,
    "failed": 0,
    "skipped": 0
  },
  "violations": [],
  "verdict": "PASS"|"FAIL"
}
```

### Red Flags (TDD Violations)

ðŸš© **Immediate Rejection:**
- Tests written AFTER implementation
- Tests that always pass (no assertions)
- Mocked everything (no real behavior tested)
- Coverage below threshold with no justification
- Tests removed to make suite pass

ðŸŸ¡ **Needs Review:**
- Coverage just barely meets threshold
- Complex tests that are hard to understand
- Tests coupled to implementation details
- Missing edge case coverage

## TDD When Delegating to Sub-Agents

### Orchestrator's Responsibility

As orchestrator, you MUST specify TDD requirements when delegating tasks.

### Template: Component Builder

```typescript
Task(subagent_type='component-builder', prompt=`
Build MetricCard component using TDD:

CRITICAL: Follow TDD cycle strictly (RED-GREEN-REFACTOR)

1. FIRST: Write component test (MetricCard.test.tsx)
   Tests to include:
   - renders title and value props
   - shows trend indicator when provided
   - displays icon when provided
   - has correct accessibility attributes (ARIA labels)
   - applies custom className

   RUN TEST: Verify test FAILS (component doesn't exist yet)
   Document failure in response

2. THEN: Implement component (MetricCard.tsx)
   - Minimal implementation to pass tests
   - TypeScript interface with all props
   - Basic JSX structure

   RUN TEST: Verify tests PASS
   Document success in response

3. FINALLY: Refactor (if needed)
   - Add styling (use design tokens)
   - Add micro-interactions (hover, transitions)
   - Add JSDoc documentation
   - Clean up code (DRY, naming)

   RUN ALL TESTS: Verify tests still PASS
   Document success in response

Return:
- Component file (MetricCard.tsx)
- Test file (MetricCard.test.tsx)
- Test execution results (RED phase output, GREEN phase output, REFACTOR phase output)
- Verification that TDD cycle was followed
`)
```

### Template: API Builder

```typescript
Task(subagent_type='api-builder', prompt=`
Implement GET /api/v1/dashboard/leads endpoint using TDD:

CRITICAL: Follow TDD cycle strictly (RED-GREEN-REFACTOR)

1. FIRST: Write API integration test (route.integration.test.ts)
   Tests to include:
   - returns 200 with data array for authenticated user
   - returns 401 for unauthenticated request
   - filters by status parameter
   - filters by type parameter
   - paginates results (limit/offset)
   - searches by query parameter

   Use pattern: Committed Data + Unique Identifiers
   (See testing patterns documentation for details)

   RUN TEST: Verify tests FAIL (route doesn't exist yet)
   Document failure in response

2. THEN: Implement API route (route.ts)
   - Minimal implementation to pass tests
   - Auth validation (requireAuth helper)
   - Query parsing and validation (Zod schema)
   - Database query with filters
   - Response formatting

   RUN TEST: Verify tests PASS
   Document success in response

3. FINALLY: Refactor (if needed)
   - Extract query builder to utility
   - Add JSDoc documentation
   - Optimize database query
   - Add error handling

   RUN ALL TESTS: Verify tests still PASS
   Document success in response

Return:
- Route file (route.ts)
- Test file (route.integration.test.ts)
- Test execution results (RED, GREEN, REFACTOR phases)
- Verification of TDD cycle
`)
```

### Template: Database Architect

```typescript
Task(subagent_type='database-architect', prompt=`
Implement database schema and migrations using TDD:

CRITICAL: Follow TDD cycle strictly (RED-GREEN-REFACTOR)

Backend (Database Schema):
1. RED: Write schema validation tests â†’ verify fails
2. GREEN: Implement schema and migrations â†’ verify passes
3. REFACTOR: Optimize indexes and constraints â†’ verify still passes

Data Access Layer:
1. RED: Write repository/query tests â†’ verify fails
2. GREEN: Implement data access methods â†’ verify passes
3. REFACTOR: Optimize queries and add caching â†’ verify still passes

Integration:
1. RED: Write integration tests with real database â†’ verify fails
2. GREEN: Connect all layers â†’ verify passes
3. REFACTOR: Clean up and optimize â†’ verify still passes

Return:
- Schema files (migrations, models)
- Data access files (repositories, queries)
- All test files (schema.test.ts, repository.test.ts, integration.test.ts)
- Test execution results for EACH TDD cycle
- Verification that TDD was followed for ALL components
`)
```
