# TDD (Test-Driven Development) - Include-Only File

<!-- WARNING: This file is for {{include-section:}} only. DO NOT read directly. -->

<!-- section: principles -->
## TDD Principles (All Roles)

Test-Driven Development is NON-NEGOTIABLE for all implementation work.

### The RED-GREEN-REFACTOR Cycle
- **RED**: Write a failing test first and confirm it fails for the right reason
- **GREEN**: Add the minimum code required to make the test passâ€”no extras
- **REFACTOR**: Improve the code with all tests green, then rerun the full suite
- Repeat the cycle for every feature/change

### The Iron Law (Stop-the-Line)
**No production code without a failing test first.**

If implementation exists before the test:
- Revert/stash the implementation, write the test first, then implement from the test.
- If you genuinely must proceed without strict test-first ordering, get explicit approval and document the rationale + follow-up task in the implementation report (do not silently skip).

### Core Rules
- Fail first; do not skip the RED step
- Minimal green code; avoid speculative features
- Refactor with a full test run before proceeding
- Coverage targets from config: overall >= {{config.quality.coverage.overall}}%, changed/new >= {{config.quality.coverage.changed}}%
- Update tests only to reflect agreed spec/format changes, never just to "make green"
- Keep output cleanâ€”no console noise

### Good Tests (Heuristics)
- One behavior per test (if the test name contains "and", split it).
- Test names describe behavior + expected outcome (avoid `test1`, `works`).
- Assert on observable outcomes (return values, state changes, HTTP responses), not internal call sequences.
- Tests should be deterministic and isolated (no shared global state, no ordering reliance).

### Guardrails
- No `.skip` / `.todo` / `.only` (or equivalents) committed
- Do not leave debugging logs in tests
- Evidence must be generated by trusted runners, not manually fabricated

### Commit Tag Requirements
Commits MUST include explicit markers to document TDD compliance:
- `[RED]` tag for commits with failing test (test written before implementation)
- `[GREEN]` tag for commits where tests pass (minimal implementation added)
- `[REFACTOR]` tag for commits with code cleanup (tests still green)
<!-- /section: principles -->

<!-- section: agent-execution -->
## TDD Execution (Agents)

### Mandatory Workflow

#### 1. RED Phase: Write Tests First
Write tests BEFORE any implementation code. Tests MUST fail initially.

**Verify RED Phase**:
```bash
{{function:ci_command("test", "<run test command from active test framework>")}}
# Expected: Test FAILS for the right reason (feature/behavior missing)
```

**RED Phase Checklist**:
- [ ] Test written BEFORE implementation
- [ ] Test fails when run (not skipped)
- [ ] Failure is an assertion/expectation failure (not a syntax/runtime error)
- [ ] Failure message is clear and points to missing behavior (not test bugs)
- [ ] Test covers the specific functionality
- [ ] If the test passes immediately, stop: tighten/adjust the test until it fails correctly (otherwise it may not be testing what you think)

#### 2. GREEN Phase: Minimal Implementation
Write the MINIMUM code needed to make the test pass.

**Verify GREEN Phase**:
```bash
{{function:ci_command("test", "<run test command from active test framework>")}}
# Expected: Test PASSES
```

**GREEN Phase Checklist**:
- [ ] Implementation makes test pass
- [ ] No extra code beyond what's needed
- [ ] Test passes consistently
- [ ] Other relevant tests still pass (no regressions introduced)

#### 3. REFACTOR Phase: Clean Up
Improve code quality while keeping tests passing.

**Verify REFACTOR Phase**:
```bash
{{function:ci_command("test", "<run test command from active test framework>")}}
# Expected: ALL tests still PASS
```

**REFACTOR Phase Checklist**:
- [ ] Code is cleaner/more readable
- [ ] Error handling added
- [ ] Validation added
- [ ] ALL tests still pass

### Common Testing Anti-Patterns (Avoid)
- Testing mock/spies/call counts as "proof" instead of asserting outcomes.
- Adding test-only methods/flags to production code to make tests easier.
- Mocking/stubbing without understanding what real side effects the test depends on.
- Boundary mocks that don't match the real schema/shape (partial mocks that silently diverge).

### Gate Checks (Before You Proceed)
**Before adding any production method to "help tests":**
- Is it used by production code (not just tests)? If not, put it in test utilities/fixtures instead.
- Does this class actually own the resource lifecycle being "cleaned up"? If not, it's the wrong place.

**Before adding any mock/double (even at boundaries):**
- What side effects does the real dependency have, and does the test rely on them?
- Can you run once with the real implementation to observe what's actually needed?
- If mocking a boundary response, mirror the full response shape/schema (not just fields the test touches).

### Evidence Requirements
- Test file created/committed BEFORE implementation file (verify via git history)
- Commits MUST include explicit markers: `[RED]` then `[GREEN]` (in order)
- RED failure documented â†’ GREEN pass documented â†’ REFACTOR documented
- Attach test output showing the failing run and the passing run
- Include a coverage report for the round
- Store evidence in the task round evidence directory using the **config-driven filenames** (e.g. `command-test.txt`, `coverage-*.txt` when configured)
- If TDD must be skipped, record the rationale in the implementation report + QA brief and create a follow-up task to add the missing tests; do not silently skip

### What NOT To Do
**NEVER**:
- Implement before writing tests
- "I'll add tests later" - NO!
- Skip test verification (RED phase must fail)
- Use excessive mocking (test real behavior)
- Leave skipped/focused/disabled tests in committed code
- Commit with failing tests

### Performance Targets
| Test Type | Target Time | Description |
|-----------|-------------|-------------|
| Unit tests | <100ms each | Pure logic, no external dependencies |
| Integration tests | <1s each | Multiple components working together |
| API/Service tests | <100ms each | Service layer with real dependencies |
| UI/Component tests | <200ms each | Rendering and interaction tests |
| End-to-End tests | <5s each | Full user journey tests |
<!-- /section: agent-execution -->

<!-- section: validator-check -->
## TDD Compliance Checking (Validators)

### Verification Checklist
- [ ] Tests exist in appropriate test directory
- [ ] Test file created BEFORE implementation (check git history)
- [ ] Tests cover the requirements specified in task

### Red Phase Evidence
- [ ] Sub-agent showed tests failing initially
- [ ] Failure messages indicate tests were actually testing something
- [ ] No skipped/disabled tests or commented-out tests

### Green Phase Evidence
- [ ] All tests now passing
- [ ] No tests were removed or weakened to pass
- [ ] Coverage meets minimum threshold

### Refactor Phase (if applicable)
- [ ] Tests still pass after refactoring
- [ ] Code is cleaner without changing behavior
- [ ] No new functionality added during refactor

### Red Flags (Immediate Rejection)
ðŸš© **Immediate Rejection:**
- Tests written AFTER implementation (check git history)
- Tests that always pass (no assertions)
- Mocked everything (no real behavior tested)
- Test-only production methods/flags added solely to enable tests
- Tests primarily assert on call counts/spies instead of observable behavior
- Coverage below threshold with no justification
- Tests removed to make suite pass

ðŸŸ¡ **Needs Review:**
- Coverage just barely meets threshold
- Complex tests that are hard to understand
- Tests coupled to implementation details
- Missing edge case coverage
- Boundary mocks/fixtures that appear incomplete or drift from real schemas
<!-- /section: validator-check -->

<!-- section: orchestrator-verify -->
## TDD Verification (Orchestrators)

### Before Accepting Work from Sub-Agent

#### 1. Test-First Evidence
- [ ] Tests exist in the appropriate test directory for the active stack (per pack conventions)
- [ ] Test file created BEFORE implementation (check git history)
- [ ] Tests cover the requirements specified in task

#### 2. Red Phase Evidence
- [ ] Sub-agent showed tests failing initially
- [ ] Failure messages indicate tests were actually testing something
- [ ] No "test.skip" or commented-out tests
- [ ] No "test passes immediately" cases without a clear explanation/fix

#### 3. Green Phase Evidence
- [ ] All tests now passing
- [ ] No tests were removed or weakened to pass
- [ ] Coverage meets minimum threshold (see quality.coverageTarget)

#### 4. Refactor Phase (if applicable)
- [ ] Tests still pass after refactoring
- [ ] Code is cleaner without changing behavior
- [ ] No new functionality added during refactor

### TDD Delegation Templates

#### Component Builder Delegation
```
Task(subagent_type='component-builder', prompt=`
Build [Component] using TDD:

CRITICAL: Follow TDD cycle strictly (RED-GREEN-REFACTOR)

1. FIRST: Write component test (`[Component].test.<ext>`)
   RUN TEST: Verify test FAILS (component doesn't exist yet)
   Document failure in response

2. THEN: Implement component (`[Component].<ext>`)
   RUN TEST: Verify tests PASS
   Document success in response

3. FINALLY: Refactor (if needed)
   RUN ALL TESTS: Verify tests still PASS
   Document success in response

Return:
- Component file
- Test file
- Test execution results (RED, GREEN, REFACTOR phases)
- Verification that TDD cycle was followed
`)
```

#### API Builder Delegation
```
Task(subagent_type='api-builder', prompt=`
Implement [endpoint] using TDD:

CRITICAL: Follow TDD cycle strictly (RED-GREEN-REFACTOR)

1. FIRST: Write API integration test
   RUN TEST: Verify tests FAIL
   Document failure in response

2. THEN: Implement API route
   RUN TEST: Verify tests PASS
   Document success in response

3. FINALLY: Refactor (if needed)
   RUN ALL TESTS: Verify tests still PASS
   Document success in response

Return:
- API files changed
- Test files
- Test execution results
- Proof of TDD compliance
`)
```
<!-- /section: orchestrator-verify -->
