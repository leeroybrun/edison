# TDD (Test-Driven Development) - Include-Only File

<!-- WARNING: This file is for {{include-section:}} only. DO NOT read directly. -->

<!-- section: principles -->
## TDD Principles (All Roles)

Test-Driven Development is NON-NEGOTIABLE for all implementation work.

### Scope: What Requires TDD (and what does not)
- **Requires TDD**: Any change that adds/changes executable behavior (production source code, CLIs, validators, state machines, config-loading/merging logic).
- **Does not require new tests**: Content-only edits to Markdown/YAML/templates (e.g., docs, templates, default config values) *when no executable behavior changes*.
- **No bundling**: Do not hide behavior changes inside a ‚Äúcontent-only‚Äù change. If you touched production code, you must follow TDD.

### The RED-GREEN-REFACTOR Cycle
- **RED**: Write a failing test first and confirm it fails for the right reason
- **GREEN**: Add the minimum code required to make the test pass‚Äîno extras
- **REFACTOR**: Improve the code with all tests green, then rerun the full suite
- Repeat the cycle for every feature/change

### The Iron Law (Stop-the-Line)
**No production code without a failing test first.**

If implementation exists before the test:
- Revert/stash the implementation, write the test first, then implement from the test.
- If you genuinely must proceed without strict test-first ordering, get explicit approval and document the rationale + follow-up task in the implementation report (do not silently skip).

### Core Rules
- Fail first; do not skip the RED step
- Minimal green code; avoid speculative features
- Refactor with a full test run before proceeding
- Coverage targets from config: overall >= {{config.quality.coverage.overall}}%, changed/new >= {{config.quality.coverage.changed}}%
- Update tests only to reflect agreed spec/format changes, never just to "make green"
- Keep output clean‚Äîno console noise

### Good Tests (Heuristics)
- One behavior per test (if the test name contains "and", split it).
- Test names describe behavior + expected outcome (avoid `test1`, `works`).
- Assert on observable outcomes (return values, state changes, HTTP responses), not internal call sequences.
- Tests should be deterministic and isolated (no shared global state, no ordering reliance).
- Avoid brittle ‚Äúcontent policing‚Äù tests (e.g., pinning default config values or exact Markdown wording/format/length).

{{include-section:guidelines/includes/TEST_SUITES.md#suite-selection}}

### Guardrails
- No `.skip` / `.todo` / `.only` (or equivalents) committed
- Do not leave debugging logs in tests
- Evidence must be generated by trusted runners, not manually fabricated

### Commit Tag Requirements
Commits MUST include explicit markers to document TDD compliance:
- `[RED]` tag for commits with failing test (test written before implementation)
- `[GREEN]` tag for commits where tests pass (minimal implementation added)
- `[REFACTOR]` tag for commits with code cleanup (tests still green)
<!-- /section: principles -->

<!-- section: agent-execution -->
## TDD Execution (Agents)

### Mandatory Workflow

#### 1. RED Phase: Write Tests First
Write tests BEFORE any implementation code. Tests MUST fail initially.
If the change is truly content-only (Markdown/YAML/templates) and no executable behavior is changed, do not add tests that pin content; just run the relevant existing checks.

**Verify RED Phase**:
```bash
{{fn:ci_command("test")}}
# Expected: Test FAILS for the right reason (feature/behavior missing)
```

**Evidence note:** failing RED runs are not ‚Äúevidence‚Äù. Evidence capture is for *passing* command outputs that validators will trust.

**RED Phase Checklist**:
- [ ] Test written BEFORE implementation
- [ ] Test fails when run (not skipped)
- [ ] Failure is an assertion/expectation failure (not a syntax/runtime error)
- [ ] Failure message is clear and points to missing behavior (not test bugs)
- [ ] Test covers the specific functionality
- [ ] If the test passes immediately, stop: tighten/adjust the test until it fails correctly (otherwise it may not be testing what you think)

#### 2. GREEN Phase: Minimal Implementation
Write the MINIMUM code needed to make the test pass.

**Verify GREEN Phase**:
```bash
{{fn:ci_command("test")}}
# Expected: Test PASSES
```

**After GREEN (when passing), capture reusable evidence** (so others can reuse results when the repo fingerprint hasn‚Äôt changed):
```bash
edison evidence capture <task-id> --only test
```

**GREEN Phase Checklist**:
- [ ] Implementation makes test pass
- [ ] No extra code beyond what's needed
- [ ] Test passes consistently
- [ ] Other relevant tests still pass (no regressions introduced)

#### 3. REFACTOR Phase: Clean Up
Improve code quality while keeping tests passing.

**Verify REFACTOR Phase**:
```bash
{{fn:ci_command("test")}}
# Expected: ALL tests still PASS
```

**After REFACTOR (when passing), refresh evidence if needed**:
```bash
edison evidence status <task-id>          # See what‚Äôs required/missing
edison evidence capture <task-id>         # Capture preset-required evidence (may reuse snapshot)
```

**REFACTOR Phase Checklist**:
- [ ] Code is cleaner/more readable
- [ ] Error handling added
- [ ] Validation added
- [ ] ALL tests still pass

### Common Testing Anti-Patterns (Avoid)
- Testing mock/spies/call counts as "proof" instead of asserting outcomes.
- Adding test-only methods/flags to production code to make tests easier.
- Mocking/stubbing without understanding what real side effects the test depends on.
- Boundary mocks that don't match the real schema/shape (partial mocks that silently diverge).

### Gate Checks (Before You Proceed)
**Before adding any production method to "help tests":**
- Is it used by production code (not just tests)? If not, put it in test utilities/fixtures instead.
- Does this class actually own the resource lifecycle being "cleaned up"? If not, it's the wrong place.

**Before adding any mock/double (even at boundaries):**
- What side effects does the real dependency have, and does the test rely on them?
- Can you run once with the real implementation to observe what's actually needed?
- If mocking a boundary response, mirror the full response shape/schema (not just fields the test touches).

### Evidence Requirements
- Test file created/committed BEFORE implementation file (verify via git history)
- Commits MUST include explicit markers: `[RED]` then `[GREEN]` (in order)
- RED failure documented ‚Üí GREEN pass documented ‚Üí REFACTOR documented
- Attach test output showing the failing run and the passing run
- Include a coverage report for the round
- Collect command evidence via `edison evidence capture <task-id>` (stores into the fingerprinted snapshot store and reuses snapshots when the repo state fingerprint is unchanged). Do **not** redirect command output into ‚Äúevidence files‚Äù manually.
- If TDD must be skipped, record the rationale in the implementation report + QA brief and create a follow-up task to add the missing tests; do not silently skip

### What NOT To Do
**NEVER**:
- Implement before writing tests
- "I'll add tests later" - NO!
- Skip test verification (RED phase must fail)
- Use excessive mocking (test real behavior)
- Leave skipped/focused/disabled tests in committed code
- Commit with failing tests

### Performance Targets
| Test Type | Target Time | Description |
|-----------|-------------|-------------|
| Unit tests | <100ms each | Pure logic, no external dependencies |
| Integration tests | <1s each | Multiple components working together |
| API/Service tests | <100ms each | Service layer with real dependencies |
| UI/Component tests | <200ms each | Rendering and interaction tests |
| End-to-End tests | <5s each | Full user journey tests |
<!-- /section: agent-execution -->

<!-- section: validator-check -->
## TDD Compliance Checking (Validators)

### Verification Checklist
- [ ] Tests exist in appropriate test directory
- [ ] Test file created BEFORE implementation (check git history)
- [ ] Tests cover the requirements specified in task
- [ ] Tests validate behavior (avoid hard-pinning default config values or enforcing exact Markdown wording/format/length)

### Red Phase Evidence
- [ ] Sub-agent showed tests failing initially
- [ ] Failure messages indicate tests were actually testing something
- [ ] No skipped/disabled tests or commented-out tests

### Green Phase Evidence
- [ ] All tests now passing
- [ ] No tests were removed or weakened to pass
- [ ] Coverage meets minimum threshold

### Refactor Phase (if applicable)
- [ ] Tests still pass after refactoring
- [ ] Code is cleaner without changing behavior
- [ ] No new functionality added during refactor

### Red Flags (Immediate Rejection)
üö© **Immediate Rejection:**
- Tests written AFTER implementation (check git history)
- Tests that always pass (no assertions)
- Mocked everything (no real behavior tested)
- Test-only production methods/flags added solely to enable tests
- Tests primarily assert on call counts/spies instead of observable behavior
- Coverage below threshold with no justification
- Tests removed to make suite pass
- Tests added/modified to enforce specific default config values or doc/template wording (brittle content gates)

üü° **Needs Review:**
- Coverage just barely meets threshold
- Complex tests that are hard to understand
- Tests coupled to implementation details
- Missing edge case coverage
- Boundary mocks/fixtures that appear incomplete or drift from real schemas
<!-- /section: validator-check -->

<!-- section: orchestrator-verify -->
## TDD Verification (Orchestrators)

### Before Accepting Work from Sub-Agent

#### 1. Test-First Evidence
- [ ] Tests exist in the appropriate test directory for the active stack (per pack conventions)
- [ ] Test file created BEFORE implementation (check git history)
- [ ] Tests cover the requirements specified in task

#### 2. Red Phase Evidence
- [ ] Sub-agent showed tests failing initially
- [ ] Failure messages indicate tests were actually testing something
- [ ] No "test.skip" or commented-out tests
- [ ] No "test passes immediately" cases without a clear explanation/fix

#### 3. Green Phase Evidence
- [ ] All tests now passing
- [ ] No tests were removed or weakened to pass
- [ ] Coverage meets minimum threshold (see quality.coverageTarget)

#### 4. Refactor Phase (if applicable)
- [ ] Tests still pass after refactoring
- [ ] Code is cleaner without changing behavior
- [ ] No new functionality added during refactor

### TDD Delegation Templates

#### Component Builder Delegation
```
Task(subagent_type='component-builder', prompt=`
Build [Component] using TDD:

CRITICAL: Follow TDD cycle strictly (RED-GREEN-REFACTOR)

1. FIRST: Write component test (`[Component].test.<ext>`)
   RUN TEST: Verify test FAILS (component doesn't exist yet)
   Document failure in response

2. THEN: Implement component (`[Component].<ext>`)
   RUN TEST: Verify tests PASS
   Document success in response

3. FINALLY: Refactor (if needed)
   RUN ALL TESTS: Verify tests still PASS
   Document success in response

Return:
- Component file
- Test file
- Test execution results (RED, GREEN, REFACTOR phases)
- Verification that TDD cycle was followed
`)
```

#### API Builder Delegation
```
Task(subagent_type='api-builder', prompt=`
Implement [endpoint] using TDD:

CRITICAL: Follow TDD cycle strictly (RED-GREEN-REFACTOR)

1. FIRST: Write API integration test
   RUN TEST: Verify tests FAIL
   Document failure in response

2. THEN: Implement API route
   RUN TEST: Verify tests PASS
   Document success in response

3. FINALLY: Refactor (if needed)
   RUN ALL TESTS: Verify tests still PASS
   Document success in response

Return:
- API files changed
- Test files
- Test execution results
- Proof of TDD compliance
`)
```
<!-- /section: orchestrator-verify -->

